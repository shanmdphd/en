<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Yixuan Qiu" />


<title>Solving Statistical Optimization Problems with the ADMM Package</title>

<script src="index_files/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="index_files/bootstrap-3.3.1/css/flatly.min.css" rel="stylesheet" />
<script src="index_files/bootstrap-3.3.1/js/bootstrap.min.js"></script>
<script src="index_files/bootstrap-3.3.1/shim/html5shiv.min.js"></script>
<script src="index_files/bootstrap-3.3.1/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="index_files/highlight/default.css"
      type="text/css" />
<script src="index_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}
</style>
<div class="container-fluid main-container">


<div id="header">
<h1 class="title">Solving Statistical Optimization Problems with the ADMM Package</h1>
<h4 class="author"><em>Yixuan Qiu</em></h4>
</div>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#algorithms">Algorithms</a></li>
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#models-and-references">Models And References</a><ul>
<li><a href="#lasso">Lasso</a></li>
<li><a href="#elastic-net">Elastic Net</a></li>
<li><a href="#dantzig-selector">Dantzig Selector</a></li>
<li><a href="#basis-pursuit">Basis Pursuit</a></li>
<li><a href="#least-absolute-deviation">Least Absolute Deviation</a></li>
</ul></li>
<li><a href="#additional-examples">Additional Examples</a><ul>
<li><a href="#least-absolute-deviation-1">Least Absolute Deviation</a></li>
<li><a href="#basis-pursuit-compressed-sensing">Basis Pursuit (Compressed Sensing)</a></li>
</ul></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p><strong>ADMM</strong> is an R package that utilizes the Alternating Direction Method of Multipliers (ADMM) algorithm to solve a broad range of statistical optimization problems. Presently the models that <strong>ADMM</strong> has implemented include:</p>
<ul>
<li>Lasso &amp; Elastic Net</li>
<li>Dantzig Selector</li>
<li>Basis Pursuit</li>
<li>Least Absolute Deviation</li>
</ul>
<p>The core part of <strong>ADMM</strong> is written in efficient C++ code, with the help of the <a href="http://eigen.tuxfamily.org">Eigen</a> library and its R wrapper <a href="http://cran.r-project.org/web/packages/RcppEigen/index.html">RcppEigen</a>. The computational performance of <strong>ADMM</strong> is comparable to the most cutting-edge R packages such as <a href="http://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a>, and outperforms most existing solvers on other models.</p>
<p>The <strong>ADMM</strong> package is especially suitable for large scale problems, in which an acceptable solution can be obtained in a few iterations for a moderate precision. It also supports parallel computing using <a href="http://openmp.org">OpenMP</a> for even bigger data.</p>
</div>
<div id="installation" class="section level1">
<h1>Installation</h1>
<p>Currently <strong>ADMM</strong> can be installed from <a href="https://github.com/yixuan/ADMM">GitHub</a> using the <a href="http://cran.r-project.org/web/packages/devtools/index.html">devtools</a> package.</p>
<pre class="r"><code># install devtools package if not present
if(!require(&quot;devtools&quot;))  install.packages(&quot;devtools&quot;)
library(devtools)
install_github(&quot;yixuan/ADMM&quot;)</code></pre>
<p><strong>ADMM</strong> relies on extension R packages <a href="http://cran.r-project.org/web/packages/RcppEigen/index.html">RcppEigen</a>, <a href="http://cran.r-project.org/web/packages/rARPACK/index.html">rARPACK</a> and <a href="http://cran.r-project.org/web/packages/ggplot2/index.html">ggplot2</a>. The <code>install_github()</code> command above will also install these dependencies if required.</p>
</div>
<div id="algorithms" class="section level1">
<h1>Algorithms</h1>
<p>The <strong>ADMM</strong> package is an implementation of the ADMM algorithm on a number of popular statistical models. The ADMM algorithm solves problems of the form</p>
<p><span class="math">\[\begin{aligned}
\text{minimize}\quad &amp; f(x)+g(z)\\
\text{subject to}\quad &amp; Ax + Bz = c
\end{aligned}\]</span></p>
<p>where <span class="math">\(x\)</span>, <span class="math">\(z\)</span> are vectors, <span class="math">\(A\)</span> and <span class="math">\(B\)</span> are matrices of suitable dimensions, and <span class="math">\(f\)</span>, <span class="math">\(g\)</span> are convex functions.</p>
<p>A wide range of statistical optimization problems can be written in this form, including the ones that this package has implemented.</p>
<p>ADMM algorithm can be expressed in the following iterative update equations:</p>
<p><span class="math">\[\begin{align}
x^{k+1} &amp; :=\underset{x}{\arg\min}\left(f(x)+\frac{\rho}{2}\Vert Ax+Bz^{k}-c+y^{k}/\rho\Vert_{2}^{2}\right)\\
z^{k+1} &amp; :=\underset{z}{\arg\min}\left(g(z)+\frac{\rho}{2}\Vert Ax^{k+1}+Bz-c+y^{k}/\rho\Vert_{2}^{2}\right)\\
y^{k+1} &amp; :=y^{k}+\rho(Ax^{k+1}+Bz^{k+1}-c)
\end{align}\]</span></p>
<p>where <span class="math">\(\rho&gt;0\)</span> is the step size parameter. More details about this algorithm can be found in the reference &lt;…&gt;.</p>
</div>
<div id="quick-start" class="section level1">
<h1>Quick Start</h1>
<p>This section provides the basic usage and flavors of the <strong>ADMM</strong> package. We will go through some simple examples to illustrate the common use of <strong>ADMM</strong> functions. More details will be given in the next section.</p>
<p>We first generate some synthetic random data for the Lasso, Elastic Net and Dantzig Selector models:</p>
<pre class="r"><code>set.seed(123)
n = 100
p = 20
nonzero = 5
b = matrix(c(runif(nonzero), rep(0, p - nonzero)))
x = matrix(rnorm(n * p, mean = 1.2, sd = 2), n, p)
y = 5 + x %*% b + rnorm(n)</code></pre>
<p>Unlike most other model building functions in R and extension packages, <strong>ADMM</strong> makes use of the Reference Class infrastructure in R to build and fit models, so that the syntax is in an Object-Oriented Programming (OOP) style. The typical way to fit an model can be expressed in the following steps:</p>
<ol style="list-style-type: decimal">
<li>Call a particular model function to create a “model object”.</li>
<li>Set necessary parameters and options through member functions of this model object.</li>
<li>Actually run the estimation procedure by calling the model fitting member function.</li>
<li>Conduct additional tasks, such as plotting and prediction.</li>
</ol>
<p>For the first step, functions calls are quite straightforward: simply provide the data matrix and response vector as arguments. The following code creates three model objects of the corresponding types.</p>
<pre class="r"><code>library(ADMM)
mod1 = admm_lasso(x, y)
mod2 = admm_enet(x, y)
mod3 = admm_dantzig(x, y)</code></pre>
<p>Note that at this stage no real computation has been conducted. The model objects are simply descriptions of the model setting, which can be modified by calling a number of member functions:</p>
<pre class="r"><code>mod2$penalty(alpha = 0.3)
mod2$opts(maxit = 1000)

mod3$penalty(lambda_min_ratio = 0.01)</code></pre>
<p>The commands above set the <span class="math">\(\alpha\)</span> parameter in the Elastic Net model to be 0.3, limit the number of iterations to be 1000, and adjust the tuning parameter sequence in the Dantzig Selector model.</p>
<p>After setting necessary parameters and options, models could be fitted by the <code>$fit()</code> member function. This is where the actual computation is done.</p>
<pre class="r"><code>fit1 = mod1$fit()
fit2 = mod2$fit()
fit3 = mod3$fit()</code></pre>
<p>Now the calculated <span class="math">\(\beta\)</span> vectors are contained in the <code>beta</code> field of the obtained results. Solution path plots can also be created by further calling the <code>$plot()</code> member function on the result objects.</p>
<pre class="r"><code>print(fit1$beta[1:6, 1:6])</code></pre>
<pre><code>## 6 x 6 sparse Matrix of class &quot;dgCMatrix&quot;
##                                                                      
## [1,] 8.641706e+00 8.53032039 8.3720347 8.2277785 8.0963507 7.93507155
## [2,] .            .          .         .         .         .         
## [3,] .            .          .         .         .         0.03183425
## [4,] .            .          .         .         .         .         
## [5,] .            0.02119865 0.1009311 0.1736284 0.2398610 0.30166664
## [6,] 3.405897e-06 0.09894514 0.1801963 0.2542077 0.3216373 0.38159870</code></pre>
<pre class="r"><code>library(ggplot2)
fit1$plot() %+% ggtitle(&quot;Solution Path for Lasso&quot;)
fit2$plot() %+% ggtitle(&quot;Solution Path for Enet&quot;)
fit3$plot() %+% ggtitle(&quot;Solution Path for Dantzig Selector&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-6-1.png" title="" alt="" width="300px" /><img src="index_files/figure-html/unnamed-chunk-6-2.png" title="" alt="" width="300px" /><img src="index_files/figure-html/unnamed-chunk-6-3.png" title="" alt="" width="300px" /></p>
<p>An appealing feature of the <strong>ADMM</strong> package is that most model building functions are “chainable”, in the sense that one member function call can be followed by another. Hence the commands above can be simplified into some shorter code:</p>
<pre class="r"><code>admm_lasso(x, y)$fit()$plot()

mod2 = admm_enet(x, y)$penalty(alpha = 0.3)$opts(maxit = 1000)
mod2$fit()$plot()

fit3 = admm_dantzig(x, y)$penalty(lambda_min_ratio = 0.01)$fit()
fit3$plot()</code></pre>
</div>
<div id="models-and-references" class="section level1">
<h1>Models And References</h1>
<p>The <strong>ADMM</strong> package has implemented a number of popular models in statistics and machine learning using the algorithm introduced above. This section summarizes the usage of various functions in <strong>ADMM</strong> that are related to specific models.</p>
<div id="lasso" class="section level2">
<h2>Lasso</h2>
<p>Lasso is a popular variable selection technique in high dimensional regression analysis, which tries to find the coefficient vector <span class="math">\(\beta\)</span> that minimizes</p>
<p><span class="math">\[\frac{1}{2n}\Vert y-X\beta\Vert_2^2+\lambda\Vert\beta\Vert_1\]</span></p>
<p>Here <span class="math">\(n\)</span> is the sample size and <span class="math">\(\lambda\)</span> is the regularization parameter that controls the sparseness of <span class="math">\(\beta\)</span>.</p>
<p>A Lasso model can be built and fitted using the functions below:</p>
<ul>
<li><code>admm_lasso(x, y, intercept, standardize, ...)</code>
<ul>
<li>This creates a model object of class <code>ADMM_Lasso</code>. It does not conduct the computation, but rather stores the parameters and settings of this model.</li>
<li><code>x</code>: Predictor data matrix</li>
<li><code>y</code>: Response Vector</li>
<li><code>intercept</code>: Whether to fit an intercept in the model. Default is <code>TRUE</code>.</li>
<li><code>standardize</code>: Whether to standardize the explanatory variables before fitting the model. Default is <code>TRUE</code>. Fitted coefficients are always returned on the original scale.</li>
</ul></li>
<li><code>model$penalty(lambda, nlambda, lambda_min_ratio, ...)</code>
<ul>
<li>This member function sets the sequence of <span class="math">\(\lambda\)</span> parameters to create a solution path of the Lasso model. Arguments of this function have similar meanings as in the <a href="http://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a> package.</li>
<li><code>model</code>: Model object, typically returned by <code>admm_lasso()</code></li>
<li><code>lambda</code>, <code>nlambda</code>, <code>lambda_min_ratio</code>: See <code>?ADMM::admm_lasso</code> for details.</li>
</ul></li>
<li><code>model$parallel(nthread, ...)</code>
<ul>
<li>This member function sets the number of threads for parallel computing.</li>
<li><code>model</code>: Model object</li>
<li><code>nthread</code>: Number of threads to be used</li>
</ul></li>
<li><code>model$opts(maxit, eps_abs, eps_rel, rho_ratio, ...)</code>
<ul>
<li>This member function sets options that relate to the ADMM algorithm</li>
<li><code>model</code>: Model object</li>
<li><code>maxit</code>: Maximum number of iterations</li>
<li><code>eps_abs</code>: Absolute tolerance parameter</li>
<li><code>eps_rel</code>: Relative tolerance parameter</li>
<li><code>rho_ratio</code>: Step size parameter in the ADMM algorithm</li>
</ul></li>
<li><code>model$fit()</code>
<ul>
<li>This member function starts the computation to fit the model, and returns an object with fields and member functions:</li>
<li><code>lambda</code>: The sequence of <span class="math">\(\lambda\)</span> to build the solution path</li>
<li><code>beta</code>: A sparse matrix containing the estimated coefficient vectors, each column for one <span class="math">\(\lambda\)</span>. Intercepts are contained in the first row.</li>
<li><code>niter</code>: Number of ADMM iterations</li>
<li><code>$plot()</code>: Member function to create plots</li>
</ul></li>
</ul>
<p>The simplest syntax to obtain the model fitting result is to call</p>
<pre class="r"><code>res = admm_lasso(x, y)$fit()</code></pre>
<p>While most member functions of class <code>ADMM_Lasso</code> return the model object itself, it is possible (and recommended) to call a sequence of member functions in a “chain style”:</p>
<pre class="r"><code>res = admm_lasso(x, y)$penalty(nlambda = 50)$parallel(2)$opts(maxit = 100)$fit()</code></pre>
<p>which is equivalent to</p>
<pre class="r"><code>model = admm_lasso(x, y)
model$penalty(nlambda = 50)
model$parallel(2)
model$opts(maxit = 100)
res = model$fit()</code></pre>
<p>Clearly, the chain style syntax is more concise and intuitive.</p>
<p>Once the model is fitted, we can create a solution path plot by calling the <code>$plot()</code> member function of <code>res</code>:</p>
<pre class="r"><code>res$plot()</code></pre>
</div>
<div id="elastic-net" class="section level2">
<h2>Elastic Net</h2>
<p>Elastic Net is an extension to the Lasso model. It seeks a coefficient vector <span class="math">\(\beta\)</span> that minimizes</p>
<p><span class="math">\[\frac{1}{2n}\Vert y-X\beta\Vert_2^2+\lambda\alpha\Vert\beta\Vert_1+\frac{\lambda(1-\alpha)}{2}\Vert\beta\Vert_2^2\]</span></p>
<p>Here <span class="math">\(n\)</span> is the sample size, <span class="math">\(\lambda\)</span> is the regularization parameter for penalty term, and <span class="math">\(\alpha\)</span> is the proportion of <span class="math">\(L_1\)</span> norm in the penalty part.</p>
<p>An Elastic Net model can be built and fitted using the functions below, much similar to the Lasso model:</p>
<ul>
<li><code>admm_enet(x, y, intercept, standardize, ...)</code>
<ul>
<li>This creates a model object of class <code>ADMM_Enet</code>. It does not conduct the computation, but rather stores the parameters and settings of this model.</li>
<li><code>x</code>, <code>y</code>, <code>intercept</code>, <code>standardize</code>: Same as those in the Lasso model</li>
</ul></li>
<li><code>model$penalty(lambda, nlambda, lambda_min_ratio, alpha, ...)</code>
<ul>
<li>This member function sets the sequence of <span class="math">\(\lambda\)</span> parameters as well as the <span class="math">\(\alpha\)</span> parameter. Arguments of this function have similar meanings as in the <a href="http://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a> package.</li>
<li><code>model</code>: Model object, typically returned by <code>admm_enet()</code></li>
<li><code>lambda</code>, <code>nlambda</code>, <code>lambda_min_ratio</code>: See <code>?ADMM::admm_enet</code> for details.</li>
<li><code>alpha</code>: Parameter to control the proportion of <span class="math">\(L_1\)</span> norm in the penalty term. <span class="math">\(\alpha = 1\)</span> corresponds to Lasso and <span class="math">\(\alpha = 2\)</span> is equivalent to <strong>Ridge Regression</strong>.</li>
</ul></li>
<li><code>model$opts(maxit, eps_abs, eps_rel, rho_ratio, ...)</code>
<ul>
<li>Sams as those in the Lasso model</li>
</ul></li>
<li><code>model$fit()</code>
<ul>
<li>Same as those in the Lasso model</li>
</ul></li>
</ul>
</div>
<div id="dantzig-selector" class="section level2">
<h2>Dantzig Selector</h2>
<p>Dantzig Selector is a variable selection technique that seeks a coefficient vector <span class="math">\(\beta\)</span> that minimizes <span class="math">\(\Vert\beta\Vert_1\)</span> subject to <span class="math">\[\Vert X&#39;(X\beta-y)\Vert_\infty \le \lambda\]</span></p>
<p>Here <span class="math">\(n\)</span> is the sample size, <span class="math">\(\lambda\)</span> is the regularization parameter, and <span class="math">\(\Vert\cdot\Vert_1\)</span>, <span class="math">\(\Vert\cdot\Vert_\infty\)</span> stand for the <span class="math">\(L_1\)</span> norm and maximum norm respectively.</p>
<p>A Dantzig Selector model can be built and fitted using the functions below, much similar to the Lasso model:</p>
<ul>
<li><code>admm_dantzig(x, y, intercept, standardize, ...)</code>
<ul>
<li>This creates a model object of class <code>ADMM_Dantzig</code>. It does not conduct the computation, but rather stores the parameters and settings of this model.</li>
<li><code>x</code>, <code>y</code>, <code>intercept</code>, <code>standardize</code>: Same as those in the Lasso model</li>
</ul></li>
<li><code>model$penalty(lambda, nlambda, lambda_min_ratio, ...)</code>
<ul>
<li>This member function sets the sequence of <span class="math">\(\lambda\)</span> parameters to create a solution path.</li>
<li><code>model</code>: Model object, typically returned by <code>admm_dantzig()</code></li>
<li><code>lambda</code>, <code>nlambda</code>, <code>lambda_min_ratio</code>: See <code>?ADMM::admm_dantzig</code> for details.</li>
</ul></li>
<li><code>model$opts(maxit, eps_abs, eps_rel, rho_ratio, ...)</code>
<ul>
<li>Sams as those in the Lasso model</li>
</ul></li>
<li><code>model$fit()</code>
<ul>
<li>Same as those in the Lasso model</li>
</ul></li>
</ul>
</div>
<div id="basis-pursuit" class="section level2">
<h2>Basis Pursuit</h2>
<p>Basis Pursuit is an optimization problem that minimizes <span class="math">\(\Vert \beta \Vert_1\)</span> subject to <span class="math">\(y=X\beta\)</span>. Here <span class="math">\(X\)</span> is an <span class="math">\(n\)</span> by <span class="math">\(p\)</span> matrix with <span class="math">\(p &gt; n\)</span>.</p>
<p>Basis Pursuit is broadly applied in <strong>Compressed Sensing</strong> to recover a sparse vector <span class="math">\(\beta\)</span> from the transformed lower dimensional vector <span class="math">\(y\)</span>.</p>
<p>A Basis Pursuit model can be built and fitted using the functions below:</p>
<ul>
<li><code>admm_bp(x, y, ...)</code>
<ul>
<li>This creates a model object of class <code>ADMM_BP</code>. It does not conduct the computation, but rather stores the parameters and settings of this model.</li>
<li><code>x</code>: The transformation matrix</li>
<li><code>y</code>: The transformed vector to recover from</li>
</ul></li>
<li><code>model$opts(maxit, eps_abs, eps_rel, rho_ratio, ...)</code>
<ul>
<li>Same as those in the Lasso model</li>
</ul></li>
<li><code>model$fit()</code>
<ul>
<li>This member function starts the computation to fit the model, and returns an object with fields and member functions:</li>
<li><code>beta</code>: The recovered <span class="math">\(\beta\)</span> vector in sparse form</li>
<li><code>niter</code>: Number of ADMM iterations</li>
<li><code>$plot()</code>: Member function to plot the coefficients against their indices</li>
</ul></li>
</ul>
</div>
<div id="least-absolute-deviation" class="section level2">
<h2>Least Absolute Deviation</h2>
<p>Least Absolute Deviation (LAD) is similar to an OLS regression model, but it minimizes the absolute deviation <span class="math">\(\Vert y-X\beta \Vert_1\)</span> instead of the sum of squares <span class="math">\(\Vert y-X\beta \Vert_2^2\)</span>. LAD is equivalent to the <strong>Median Regression</strong>, a special case of the quantile regression models. LAD is a robust regression technique in the sense that the estimated coefficients are insensitive to outliers.</p>
<p>A Least Absolute Deviation model can be built and fitted using the functions below:</p>
<ul>
<li><code>admm_lad(x, y, intercept, ...)</code>
<ul>
<li>This creates a model object of class <code>ADMM_LAD</code>. It does not conduct the computation, but rather stores the parameters and settings of this model.</li>
<li><code>x</code>: Predictor data matrix</li>
<li><code>y</code>: Response vector</li>
<li><code>intercept</code>: Whether to include an intercept term. Default is <code>TRUE</code>.</li>
</ul></li>
<li><code>model$opts(maxit, eps_abs, eps_rel, rho_ratio, ...)</code>
<ul>
<li>Same as those in the Lasso model</li>
</ul></li>
<li><code>model$fit()</code>
<ul>
<li>This member function starts the computation to fit the model, and returns an object with fields and member functions:</li>
<li><code>beta</code>: The estimated regression coefficients, including the intercept.</li>
<li><code>niter</code>: Number of ADMM iterations</li>
</ul></li>
</ul>
</div>
</div>
<div id="additional-examples" class="section level1">
<h1>Additional Examples</h1>
<p>Simple examples have been given for Lasso, Elastic Net and Dantzig Selector in the <strong>Quick Start</strong> section. This part provides illustrations of the LAD and Basis Pursuit models.</p>
<div id="least-absolute-deviation-1" class="section level2">
<h2>Least Absolute Deviation</h2>
<p>As is introduced in the last section, LAD is a type of robust regression technique. To gain more sense about this feature, we consider a small data set with one independent variable <span class="math">\(x\)</span> and one response variable <span class="math">\(y\)</span>, with the true regression function <span class="math">\(y=-3+2x+\varepsilon\)</span>.</p>
<p>We intentionally add an outlier in this set of data, and compare the results of OLS and LAD.</p>
<pre class="r"><code>library(ADMM)
library(ggplot2)
set.seed(123)
x = sort(rnorm(100))
y = -3 + 2 * x + rnorm(100, sd = 0.5)
y[1] = y[1] + 10
dat = data.frame(x = x, y = y)
ggplot(dat, aes(x = x, y = y)) + geom_point()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-12-1.png" title="" alt="" width="500px" style="display: block; margin: auto;" /></p>
<p>The coefficient vector estimated by OLS and LAD can be obtained using the following code:</p>
<pre class="r"><code>coef_truth = c(-3, 2)
coef_ols = coef(lm(y ~ x))
coef_lad = admm_lad(x, y)$fit()$beta</code></pre>
<p>Then the regression lines obtained by the two methods can be visualized together:</p>
<pre class="r"><code>res = rbind(coef_truth, coef_ols, coef_lad)
colnames(res) = c(&quot;intercept&quot;, &quot;slope&quot;)
res = as.data.frame(res)
res$method = c(&quot;Truth&quot;, &quot;OLS&quot;, &quot;LAD&quot;)
ggplot(dat, aes(x = x, y = y)) +
    geom_point() +
    geom_abline(aes(intercept = intercept, slope = slope, color = method),
                data = res, show_guide = TRUE)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-14-1.png" title="" alt="" width="500px" style="display: block; margin: auto;" /></p>
<p>Now it is clear that the OLS regression line is much influenced by the outlier, while LAD is resistent to that and gives result very close to the truth.</p>
</div>
<div id="basis-pursuit-compressed-sensing" class="section level2">
<h2>Basis Pursuit (Compressed Sensing)</h2>
<p>Assume that <span class="math">\(\beta\)</span> is a vector of length <span class="math">\(n\)</span> that has many elements close to zero. Let <span class="math">\(X\)</span> be a matrix of dimension <span class="math">\(m\times n\)</span> and let <span class="math">\(y=X\beta\)</span>, then <span class="math">\(y\)</span> is a linear transformation of <span class="math">\(\beta\)</span>. If <span class="math">\(m&lt;n\)</span>, it is easy to find that <span class="math">\(y\)</span> has a lower dimension than <span class="math">\(\beta\)</span>, which we can think of as a compressed version of <span class="math">\(\beta\)</span>. The theory of Compressed Sensing claims that when <span class="math">\(X\)</span> and <span class="math">\(\beta\)</span> satisfy a number of conditions, <span class="math">\(\beta\)</span> can actually be recovered from <span class="math">\(y\)</span> with a high precision, and the decompression procedure is just the problem of Basis Pursuit.</p>
<p>To illustrate this compress-recover process, we use the portrait image of R. A. Fisher as an example.</p>
<div align="center">
<img src="index_files/figure-html/RAFisher.jpg" alt="Fisher" width="300px" />
</div>
<p>We read in this image in R and preprocess each column using Discrete Cosine Transform (DCT) to obtain the original signal data:</p>
<pre class="r"><code>library(ADMM)
library(jpeg)
library(fftw)
img = readJPEG(&quot;RAFisher.jpg&quot;)
nr = nrow(img)
nc = ncol(img)

## preprocess the image using DCT
dat = matrix(0, nr, nc)
for(i in 1:nc)
{
    dat[, i] = DCT(img[, i]) / nr
}</code></pre>
<p>Then we use a random matrix whose elements are i.i.d. <span class="math">\(N(0, 1)\)</span> random numbers to compress each column of the signal data.</p>
<pre class="r"><code>set.seed(123)
m = nr / 5                    ## compressed dimension
X = matrix(rnorm(m * nr), m)  ## transformation matrix
Y = X %*% dat                 ## compressed data</code></pre>
<p>The <code>m</code> parameter is the dimension after compression, and in this case the compressed matrix is only 1/5 the size of the original data. Next, we use the Basis Pursuit solver in <strong>ADMM</strong> package to recover signals from the compressed data <code>Y</code>.</p>
<pre class="r"><code>recdat = matrix(0, nr, nc)    ## data to be recovered
for(i in 1:nc)
{
    print(i)
    recdat[, i] = as.numeric(admm_bp(X, Y[, i])$opts(eps_rel = 1e-3)$fit()$beta)
}

## restore image data using inverse DCT
recimg = matrix(0, nr, nc)
for(i in 1:nc)
{
    recimg[, i] = IDCT(recdat[, i] * nr)
}
recimg[recimg &lt; 0] = 0
recimg[recimg &gt; 1] = 1

writeJPEG(recimg, &quot;recover.jpg&quot;, quality = 0.9)</code></pre>
<p>The picture on the left is the recovered image with a compression ratio of 20% (meaning the compressed data are only 20% the size of the original one), and the image on the right has a compression ratio of 50%.</p>
<div align="center">
<img src="index_files/figure-html/rec_20.jpg" alt="Fisher-20" width="300px" /> <img src="index_files/figure-html/rec_50.jpg" alt="Fisher-50" width="300px" />
</div>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
